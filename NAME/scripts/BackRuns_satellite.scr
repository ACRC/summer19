#!/bin/ksh
# ******************************************************************************
#
# Project: Template files and scripts for research users on NAME-JASMIN
#
# File:    Sample script for NAME back runs for aircraft
#
# Author:  Andrew Jones, Atmospheric Dispersion, UK Met Office
#	   modified by Anita Ganesan & Rachel Tunnicliffe, University of Bristol
#
# Date:    25/09/2014
#          Satellite modifications 31/3/2015
#
# ******************************************************************************

# Summary of set up for back runs:
#
# - A separate NAME back run is performed for each day of satellite retrievals over a domain
# - File name format: *_Date.csv or *_Date-ID.csv e.g. GOSAT-BRAZIL_20080101.csv or GOSAT-BRAZIL_20080101-001.csv

# Inputs:
# - A file that contains the following 8 columns: ID number, timestamp, lon, d_lon, lat, d_lat, alt (Pa),d_alt (Pa)
# - The first line must be a header line
#	ID - vertical level of the footprint (1-20 for GOSAT)
#	timestamp - timestamp of the retrievals in the format YYYY-mm-DD HH:MM:SS 
#	lon - mean longitude of retrieval
#	d_lon - the range of lon in retrieval (x_max - x_min)
#	lat - mean latitude of retrieval
#	d_lat - the range of lat in retrieval (y_max - y_min)
#	alt - mean altitude of retrieval (in Pa)
#	d_alt - range of altitude in retrieval (z_max - z_min)

# Run as:
# $ ./BackRuns_satellite_general.scr [-n nthread -l LevelSplit -d MaxAge_Days] data_file Domain FolderName 
# data_file - input filename of satellite measurement times and locations as described above
# Domain - NAME domain (should cover data within data_file)
# FolderName - folder name for the output, normally of the format "Domain"_"Site"_"Height" e.g. SOUTHAMERICA_GOSAT-BRAZIL_column
# LevelSplit, nthread and MaxAge_Days are optional

# Example syntax to launch:
# $ ./BackRuns_satellite.scr GOSAT/GOSAT_20120801.csv SOUTHAMERICA SOUTHAMERICA_GOSAT-BRAZIL_column
# Use a wrapper script to launch multiple days (i.e., loop through dates each pointing to a data file)

# Outputs:
# - Outputs are stored in daily Fields files, with the number of columns in the file corresponding to the number of spatial locations times the number of vertical levels
# - Met data is stored in a seperate folder with one file per location per level
# - A particle location file for each daily run, which contains the x,y,z,t of a particle when it dies (e.g., leaves the domain or end of run)
#
# - each back run uses GLOBAL NWP analysis data (with update-on-demand)
#
#
# Update History:
#
# 08/10/2013 - original version of script supplied to Ag Stephens at BADC
#
# 26/11/2013 - version 2 includes automatic specification of the GLOBAL NWP data
#              to use based on the dates for each NAME simulation (currently the
#              start date and end date of each NAME run must be covered by the
#              same 'Mk' data, although this restriction might be relaxed later)
#
# 25/09/2014 - updated to include 'Mk8' GLOBAL UM option to be used for dates
#              from 15/07/2014 onwards.
#
# 31/3/2015 - updated to work on satellite retrievals
#
# 20/11/2015 - updated to NAME v6.5 and to include support for 'Mk9' GLOBAL UM
#              option to be used for dates from 25/08/2015 onwards.
#
# 08/05/2017 - updated to add GLOUM6 met data ('Mk3') prior to 01/01/2009
#
#
# IMPORTANT NOTES:
# 1/2/2015
# - Script only works when the entirety of the run is in the same MK version.
# - Met output currently only works for a maximum flight length of 6 hours. If longer duration is needed, then the size of HGrid, ZGrid and TGrid
#   needs to be changed in the file GlobalParamaters.F90 and model needs to be recompiled.

# Run as:
# >> ./BackRuns_satellite.scr [-s SETUP_FILE -n nthread -l LevelSplit -d MaxAge_Days] data_file Domain FolderName 
# SETUP_FILE, LevelSplit, nthread and MaxAge_Days are optional

echo
echo "Script $0 starting at" `date`

# Set system variables

. /etc/profile

ulimit -s unlimited

# This line specifically works in korn shell as opposed to alternative syntax in bash.
CURRENTDIR="$(dirname $(realpath $0))"
SCRIPTDIR=$CURRENTDIR

# ------------------------------------------------------------------------------
#
#  **  Script parameters and arguments  **
#
# ------------------------------------------------------------------------------

# Correct usage string (for display in error messages)
Usage="Usage: $0  [ -s setupFile  -n nthread  -l levelSplit  -d maxAgeDays  -h ]  dataFile  domain  folderName"

echo 
echo "Reading in command line inputs"

# Get optional parameters

## Setting default values
# DEFAULT: SETUP FILE - DEFAULT DEFINED WITHIN SECTION 1) 
# DEFAULT: NUMBER OF THREADS FOR PARALLELISED NAME EXECUTION
nThreads=1
# DEFAULT: DURATION OF SIMULATION (i.e. MAX PARTICLE AGE IN DAYS)
MaxAge_Days=30
# DEFAULT: LEVEL TO START RELEASING LESS PARTICLES (SAVES ON COMPUTING POWER)
LevelSplit=8

while getopts ":s:n:l:d:h" name; do
case "${name}" in
	"s") SETUP_FILE=$OPTARG; echo "Using setup file : $SETUP_FILE";;
    "n") nThreads=$OPTARG; echo "Updating number of parallel threads : $nThreads";;
    "l") LevelSplit=$OPTARG; echo "Updating level for particle number decrease : $LevelSplit";;
    "d") MaxAge_Days=$OPTARG; echo "Updating max particle age : $MaxAge_Days days";;
    "h") # option: help
      echo '---------------------------------------------------'
      echo ${Usage}
      echo '---------------------------------------------------'
      echo ''
      echo 'Required parameters:'
      echo '   data_filename, domain and folder_name'
      echo ''
      echo 'Optional parameters:'
      echo '  -h  displays this help page'
      echo '  -s  file containing parameter setup for the run'
      echo '  -n  number of threads'
      echo '  -l  level to reduce number of particles being released'
      echo '  -d  maximum particle age (in days)'
      echo ''
      echo '---------------------------------------------------'
      exit 0
      ;;
    "?") # illegal option
      echo 'Error: illegal option -'${OPTARG}
      echo ${Usage}
      exit 1
      ;;
esac
done

#  Get input arguments

data_file=${@:$OPTIND:1}
Domain=${@:$OPTIND+1:1}
FolderName=${@:$OPTIND+2:1}

if [ -z $data_file ] || [ -z $Domain ] || [ -z $FolderName ]
then
  echo "Error in script $0: Input data file, domain and output folder name must be specified on input."
  echo ${Usage}
  exit 1
fi	

echo "Data file        : $data_file"
echo "Domain           : $Domain"
echo "Output folder    : $FolderName"


# ------------------------------------------------------------------------------
#
#  **  User-specified parameters  **
#
# ------------------------------------------------------------------------------


# ----------------------------------------------------------------------
#  1)   SET DIRECTORIES - EXTRACTED FROM SETUP FILE
# ----------------------------------------------------------------------

# Read directory structure from a setup file specific to the system.
if [ -z $SETUP_FILE ]
then
	# If no setup file has been specified on the command line use the default
	# - File named "BackRuns_Setup.txt" within same directory as this script.
	SETUP_FILE="$CURRENTDIR/BackRuns_Setup.txt"
elif [[ ! $SETUP_FILE = "*/*" ]]
then
	# If no explicit directory structure is included, add path to current directory.
	SETUP_FILE="$CURRENTDIR/$SETUP_FILE"
fi

if [ ! -e $SETUP_FILE ]
then
	echo "ERROR: Unable to find file for setting up directory structure: $SETUP_FILE"
	exit 1
fi

source $SETUP_FILE

# Expected inputs from setup file (at the moment) - directory structures
if [ -z $NAMEIIIDIR ] || [ -z $RESULTDIR ] || [ -z $METDIR ] || [ -z $TOPOGDIR ]
then
	echo "ERROR: Not all paths defined within setup file $SETUP_FILE"
	echo "NAMEIIIDIR : $NAMEIIIDIR"
	echo "RESULTDIR  : $RESULTDIR"
	echo "METDIR     : $METDIR"
	echo "TOPOGDIR   : $TOPOGDIR"
	exit 1
fi

current_FP=$(echo $data_file | rev | cut -d'_' -f 1 | rev | cut -d'.' -f 1)
RunName=${FolderName}_${current_FP}

WORKDIR="${RESULTDIR}/${RunName}"

echo
echo "Setting up output paths"
echo "Full output path   : ${RESULTDIR}/${FolderName}"
echo "Temporary met path : $METDIR"

# ----------------------------------------------------------------------------
#  2)   OUTPUT GRIDS  AND  MODELLING DOMAIN - EXTRACTED FROM INPUT DOMAIN FILE
# ----------------------------------------------------------------------------

# Domain has been defined on input, domain directory (containing domain definitions)
# could be defined within setup file.
if [ -z $DOMAINDIR ]
then
	# If domain directory has not been defined within setup file use the default:
	# - Directory called "DomainDefn" within the same directory as this script
	DOMAINDIR="$CURRENTDIR/DomainDefn"
elsemkd
	echo
	echo "Using Domain Directory defined within $SETUP_FILE"
	echo $DOMAINDIR
fi
DomainFile="$DOMAINDIR/$Domain.txt"

# Checking folder containing domain definitions exists
if [ ! -d $DomainDefn ]
then
	echo "Unable to locate $DomainDefn directory"
	exit 1
fi

# Checking domain file exists and sourcing the file if present.
# Domain file should be called "Domain".txt e.g. SOUTHAMERICA.txt and should contain x,y grid definitions for the domain.
if [ -e $DomainFile ]
then
	echo
	echo "Sourcing domain file $DomainFile"
	source $DomainFile
else
	echo
	echo "Could not find domain file: $DomainFile"
	exit 1
fi

# Check necessary parameters for the domain Grid have been defined within the sourced file.
if [ -z $Grid1_nX ] || [ -z $Grid1_nY ] || [ -z $Grid1_dX ] || [ -z $Grid1_dY ] || [ -z $Grid1_Xmin ] || [ -z $Grid1_Ymin ]
then
	echo "Unable to define grid from $DomainFile"
	echo "Grid1_nX   : $Grid1_nX"
	echo "Grid1_nY   : $Grid1_nY"mkdir
	echo "Grid1_dX   : $Grid1_dX"
	echo "Grid1_dY   : $Grid1_dY"
	echo "Grid1_Xmin : $Grid1_Xmin"
	echo "Grid1_Ymin : $Grid1_Ymin"
	exit 1
else
	echo "Grid1_nX     : $Grid1_nX"
	echo "Grid1_nY     : $Grid1_nY"
	echo "Grid1_dX     : $Grid1_dX"
	echo "Grid1_dY     : $Grid1_dY"
	echo "Grid1_Xmin   : $Grid1_Xmin"
	echo "Grid1_Ymin   : $Grid1_Ymin"
fi

# If Computational domain has not been defined as part of the Domain definition, this can be defined
# using the recommended size of +5 degrees longitude and +4 degrees latitude.
if [ -z $CompDom_Xmin ]
then
	echo "Calculating Computational Domain Xmin based on recommendation of -5 degrees"
	CompDom_Xmin=$(expr $Grid1_Xmin-5 | bc)
fi

if [ -z $CompDom_Xmax ]
then
	echo "Calculating Computational Domain Xmax based on recommendation of +5 degrees"
	Grid1_Xmax=$(expr $Grid1_Xmin+$Grid1_nX*$Grid1_dX | bc)
	CompDom_Xmax=$(expr $Grid1_Xmax+5 | bc)
fi

if [ -z $CompDom_Ymin ]
then
	echo "Calculating Computational Domain Ymin based on recommendation of -4 degrees"
	CompDom_Ymin=$(expr $Grid1_Ymin-4 | bc)
fi

if [ -z $CompDom_Ymax ]
then
	echo "Calculating Computational Domain Ymax based on recommendation of +4 degrees"
	Grid1_Ymax=$(expr $Grid1_Ymin+$Grid1_nY*$Grid1_dY | bc)
	CompDom_Ymax=$(expr $Grid1_Ymax+5 | bc)
fi

echo "CompDom_Xmin : $CompDom_Xmin"
echo "CompDom_Xmax : $CompDom_Xmax"
echo "CompDom_Ymin : $CompDom_Ymin"
echo "CompDom_Ymax : $CompDom_Ymax"

# ----------------------------------------------------------------------
#  3)   NWP METEOROLOGY
# ----------------------------------------------------------------------

# Retention period for local NWP files
MetHistory=32

# Directory containing topography files
TopogSubdir='UMTopogData'

# ----------------------------------------------------------------------
#  4)   PARTICLE RELEASE AND SYNC TIME CHARACTERISTICS
# ----------------------------------------------------------------------

retrieval_secs=60 # duration of each satellite retrieval
MaxNumParticles=7000000

SyncTime_Minutes=06
nIntTimesPerHour=10

# ParticlesPerSource defined based on level being simulated.

# ----------------------------------------------------------------------
#  5)   CONTROL FLAGS FOR SCRIPT EXECUTION
# ----------------------------------------------------------------------

# Number of threads for parallelised NAME execution for met data (fixed to 1 for satellite)
# nThreads for NAME run set 

nThreads_met=1

# ------------------------------------------------------------------------------
#
#  **  Functions  **
#
# ------------------------------------------------------------------------------

# Gets the Mk (version number) of Global met data appropriate to a given date

get_Mk_Global(){

  # Input date as seconds from Jan 1, 1970
  
  if [[ ${GlobalMetDate} -lt 1136073600 ]] ; then
    cd site_bet
    let GlobalMetMk=0
    echo "ERROR in get_Mk_Global: date is before the earliest available Global met data"
    exit 1
    
  elif [[ ${GlobalMetDate} -lt 1230768000 ]] ; then
    
    let GlobalMetMk=3
    
  elif [[ ${GlobalMetDate} -lt 1257811200 ]] ; then
    
    let GlobalMetMk=4
    
  elif [[ ${GlobalMetDate} -lt 1268092800 ]] ; then
    
    let GlobalMetMk=5
    
  elif [[ ${GlobalMetDate} -lt 1367280000 ]] ; then
    
    let GlobalMetMk=6
    
  elif [[ ${GlobalMetDate} -lt 1405382400 ]] ; then
    
    let GlobalMetMk=7
    
  elif [[ ${GlobalMetDate} -lt 1440460800 ]] ; then
    
    let GlobalMetMk=8
    
  elif [[ ${GlobalMetDate} -lt 1499731200 ]] ; then
  
    let GlobalMetMk=9
    
  else 
  
    let GlobalMetMk=10
    
  fi

}

# ------------------------------------------------------------------------------
#
#  **  NWP specifications - these are fixed and should not be changed **
#
# ------------------------------------------------------------------------------

# Met Type             |   Dates
# -------------------------------------------------
#
# GLOUM6               |  01/01/2006 --> 31/12/2008
#
# GLOUM6pp             |  01/01/2009 --> 09/11/2009
#
# UMG_Mk5              |  10/11/2009 --> 08/03/2010
#
# UMG_Mk6              |  09/03/2010 --> 29/04/2013
#
# UMG_Mk7              |  30/04/2013 --> 14/07/2014
#
# UMG_Mk8              |  15/07/2014 --> 25/08/2015
#
# UMG_Mk9              |  25/08/2015 --> 11/07/2017
#
# UMG_Mk10             |  11/07/2017 --> ---
#

set -A MetMk             0       1       2       3                  4                    5                           6                             7                             8                             9                            10
set -A MetType           'null'  'null'  'null'  'GLOUM6'           'GLOUM6pp'           'UMG_Mk5'                   'UMG_Mk6PT'                   'UMG_Mk7PT'                   'UMG_Mk8PT'                   'UMG_Mk9PT'                  'UMG_Mk10PT'
set -A MetDefnFileName   'null'  'null'  'null'  'MetDefnUM6G.txt'  'MetDefnUM6Gpp.txt'  'MetDefnUMG_Mk5_L52pp.txt'  'MetDefnUMG_Mk6_L59PTpp.txt'  'MetDefnUMG_Mk7_L59PTpp.txt'  'MetDefnUMG_Mk8_L59PTpp.txt'  'MetDefnUMG_Mk9_L59PTpp.txt' 'MetDefnUMG_Mk10_L59PTpp.txt'
set -A MetDeclnFileName  'null'  'null'  'null'  'Use_UM6G.txt'     'Use_UM6Gpp.txt'     'Use_UMG_Mk5_L52pp.txt'     'Use_UMG_Mk6_L59PTpp.txt'     'Use_UMG_Mk7_L59PTpp.txt'     'Use_UMG_Mk8_L59PTpp.txt'     'Use_UMG_Mk9_L59PTpp.txt'    'Use_UMG_Mk10_L59PTpp.txt'
set -A MetPrefix         'null'  'null'  'null'  'HP'               'HP'                 'MO'                        'MO'                          'MO'                          'MO'                          'MO'                         'MO'
set -A MetSuffix         'null'  'null'  'null'  'GLOUM6'           'GLOUM6.pp'          'UMG_Mk5_L52.pp'            'UMG_Mk6_L59PT*.pp'           'UMG_Mk7_[IM]_L59PT*.pp'      'UMG_Mk8_[IM]_L59PT*.pp'      'UMG_Mk9_[IM]_L59PT*.pp'     'UMG_Mk10_[IM]_L59PT*.pp'
set -A ArchiveMetDir     'null'  'null'  'null'  'Global/GLOUM6'    'Global/GLOUM6pp'    'Global/UMG_Mk5'            'Global/UMG_Mk6PT'            'Global/UMG_Mk7PT'            'Global/UMG_Mk8PT'            'Global/UMG_Mk9PT'           'Global/UMG_Mk10PT'



# ------------------------------------------------------------------------------
#
#  **  Script parameters and arguments  **
#
# ------------------------------------------------------------------------------

# find out number of lines in data file (i.e., number of runs in the day)
numdata=$(wc -l < ${data_file})

if [[ ${numdata} -gt 1 ]] ; then # skip the day if there is no data

# ------------------------------------------------------------------------------
#
#  Set up directory structures (if necessary) and switch to working directory
#
# ------------------------------------------------------------------------------

# Create working directory for NAME runs

mkdir -p ${WORKDIR}

# Create local met directory

mkdir -p ${METDIR}

# Switch to working directory

cd ${WORKDIR}

# ------------------------------------------------------------------------------
#
#  Calculate input file parameters from supplied user inputs
#
# ------------------------------------------------------------------------------

# max particle age in hours
let MaxAge_Hours=${MaxAge_Days}*24

# duration of run (24 hours longer than max particle age)
#let RunDuration=${MaxAge_Hours}+24
let RunDuration=${MaxAge_Hours}+1

# total number of integration steps for integrated air concentrations
let nIntTimes=${RunDuration}*${nIntTimesPerHour}

#---------------------------------PREPARE MAIN PARTS OF INPUT FILES---------------------------------

# Met input file
met_input_file="BackRun_${RunName}_met.txt"
cp  ${SCRIPTDIR}/Met_satellite_template.txt  ${met_input_file}

# find date of run
sed -n "2p" ${data_file} > temp.out
IFS=","
while read f1 f2 f3 f4 f5 f6 f7 f8
  do
  cur_date=$f2
done < temp.out
rm temp.out
cur_date_day=`date -u -d "${cur_date} UTC" +'%Y-%m-%d %H:%M:%S'`
cur_date=`date -u -d "${cur_date_day} UTC" +'%Y-%m-%d %H:%M:%S'` 

cur_date_as_char=`date -u -d "${cur_date_day}" +'%Y%m%d%H%M%S'`  
  
# set start and end times for NAME run
# backwards run - so start of run is chronologically later than end of run
RunStartTime=`date -u -d "${cur_date} UTC              +10 sec" +'%d/%m/%Y %H:%M:%S'`
RunStopTime=`date -u -d "${cur_date} UTC -${MaxAge_Days} day" +'%d/%m/%Y %H:%M:%S'`

# ... and in seconds format as ...
RunStartSecs=`date -u -d "${cur_date} UTC              +10 sec" +'%s'`
RunStopSecs=`date -u -d "${cur_date} UTC -${MaxAge_Days} day" +'%s'`

# next determine the 'Mk' of the Global met data
GlobalMetDate=${RunStartSecs}
get_Mk_Global
RunStartMk=${GlobalMetMk}
  
#  GlobalMetDate=${RunStopSecs}
#  get_Mk_Global
#  RunStopMk=${GlobalMetMk}
  
# 'Mk' should be the same throughout the NAME run
# (might relax this condition later on, but this would make coding more complicated)
#  if [[ ${RunStartMk} -eq ${RunStopMk} ]] ; then
  GlobalMk=${RunStartMk}
#  else
#    echo "Error in script $0: the start and stop dates of the NAME run do not use the same 'Mk' Global met data"
#    exit 2
#  fi
  
# set files for use of NWP meteorology
MetDefnFile="${NAMEIIIDIR}/Resources/Defns/${MetDefnFileName[${GlobalMk}]}"
MetDeclnFile="${SCRIPTDIR}/MetDeclarations/${MetDeclnFileName[${GlobalMk}]}"
MetRestoreScript="${SCRIPTDIR}/MetRestore_JASMIN.ksh"

# set input filename for NAME run
input_file="BackRun_${RunName}.txt"

# set error filename for NAME run
error_file="BackRun_${RunName}Error.txt"

# ----------------------------------------------------------------------------
# Prepare main part of the input file
# ----------------------------------------------------------------------------

# copy main input file template for processing
cp  ${SCRIPTDIR}/BackRuns_satellite_template.txt  ${input_file}

# substitute dynamic variables in the main part of the input file

# - name of back run
sed -i "s|%Run_Name%|${RunName}|g"             ${input_file}

# - number of threads to use
sed -i "s|%nThreads%|${nThreads}|g"            ${input_file}

# - output directory
sed -i "s|%OutputDir%|${WORKDIR}|g"            ${input_file}
 
# - name of met defn file to read
sed -i "s|%MetDefnFile%|${MetDefnFile}|g"      ${input_file}

# - duration of back run in hours
sed -i "s|%RunDuration%|${RunDuration}|g"      ${input_file}

# - start and end time
sed -i "s|%StartTimeOfRun%|${RunStartTime}|g"  ${input_file}
sed -i "s|%EndTimeOfRun%|${RunStopTime}|g"     ${input_file}
  
# - sync time step (main model time step) in minutes
sed -i "s|%SyncTime%|${SyncTime_Minutes}|g"    ${input_file}

# - grid 1 definition
sed -i "s|%Grid1_nX%|${Grid1_nX}|g"            ${input_file}
sed -i "s|%Grid1_nY%|${Grid1_nY}|g"            ${input_file}
sed -i "s|%Grid1_dX%|${Grid1_dX}|g"            ${input_file}
sed -i "s|%Grid1_dY%|${Grid1_dY}|g"            ${input_file}
sed -i "s|%Grid1_Xmin%|${Grid1_Xmin}|g"        ${input_file}
sed -i "s|%Grid1_Ymin%|${Grid1_Ymin}|g"        ${input_file}

# - computational domain
sed -i "s|%CompDom_Xmin%|${CompDom_Xmin}|g"    ${input_file}
sed -i "s|%CompDom_Xmax%|${CompDom_Xmax}|g"    ${input_file}
sed -i "s|%CompDom_Ymin%|${CompDom_Ymin}|g"    ${input_file}
sed -i "s|%CompDom_Ymax%|${CompDom_Ymax}|g"    ${input_file}

# - maximum number of particles allowed
sed -i "s|%MaxNumParticles%|${MaxNumParticles}|g"  ${input_file}

# - name of back run
sed -i "s|%Run_Name%|${RunName}|g"             ${met_input_file}

# - number of threads to use
sed -i "s|%nThreads%|${nThreads_met}|g"        ${met_input_file}

# - output directory
sed -i "s|%OutputDir%|${WORKDIR}|g"            ${met_input_file}

# - name of met defn file to read
sed -i "s|%MetDefnFile%|${MetDefnFile}|g"      ${met_input_file}

sed -i "s|%SyncTime%|${SyncTime_Minutes}|g"    ${met_input_file}
  
sed -i "s|%CompDom_Xmin%|${CompDom_Xmin}|g"    ${met_input_file}
sed -i "s|%CompDom_Xmax%|${CompDom_Xmax}|g"    ${met_input_file}
sed -i "s|%CompDom_Ymin%|${CompDom_Ymin}|g"    ${met_input_file}
sed -i "s|%CompDom_Ymax%|${CompDom_Ymax}|g"    ${met_input_file}
sed -i "s|%CompDom_Ymax%|${CompDom_Ymax}|g"    ${met_input_file}

sed -i "s|%StopTimeOfFlight%|${RunStartTime}|g"  ${met_input_file}
sed -i "s|%StartTimeOfFlight%|${RunStopTime}|g"     ${met_input_file}
 

# ------------------------------------------------------------------------------
#
#  Loop over each line of the data file
#  
# ------------------------------------------------------------------------------

let SamplingPeriod_Index=0

for current_index in {2..${numdata}}
do
 # read in flight data file corresponding to current line 
  sed -n "${current_index}p" ${data_file} > temp.out
  IFS=","
  while read f1 f2 f3 f4 f5 f6 f7 f8
    do
    label=$f1
#   vert_level=$f1
    vert_level=${f1: -2}
    cur_date=$f2
    SourceLoc_X=$f3
    delta_x=$f4
    SourceLoc_Y=$f5
    delta_y=$f6
    SourceLoc_Z=$f7
    delta_z=$f8
  done < temp.out
  rm temp.out

  cur_date_day=`date -u -d "${cur_date} UTC" +'%Y-%m-%d %H:%M:%S'`
  cur_date=`date -u -d "${cur_date} UTC" +'%Y-%m-%d %H:%M:%S'`    
    
  if [[ ${vert_level} -le ${LevelSplit} ]] ; then
#  if [[ ${vert_level} -le 8 ]] ; then
    ParticlesPerSource='30/sec'
  else
    ParticlesPerSource='3/sec'
  fi
    
  let SamplingPeriod_Index=${SamplingPeriod_Index}+1  

  SamplingPeriod_Stop=`date -u -d "${cur_date} UTC" +'%d/%m/%Y %H:%M:%S'`
  SamplingPeriod_Stop_as_char=`date -u -d "${cur_date} UTC" +'%Y%m%d'`
  SamplingPeriod_Start=`date -u -d "${cur_date} UTC +${retrieval_secs} sec" +'%d/%m/%Y %H:%M:%S'`
     
  # copy template for source and output requests block for processing
  cp  "${SCRIPTDIR}/SourceTermAndOutputRequests_satellite.txt"  SourceAndRequests.tmp
  
  # substitute dynamic variables for this sampling period
    # - source location (as lat-long)
  sed -i "s|%SourceLoc_X%|${SourceLoc_X}|g"      SourceAndRequests.tmp
  sed -i "s|%SourceLoc_Y%|${SourceLoc_Y}|g"      SourceAndRequests.tmp 
  sed -i "s|%SourceLoc_Z%|${SourceLoc_Z}|g"      SourceAndRequests.tmp
  sed -i "s|%delta_x%|${delta_x}|g"      SourceAndRequests.tmp
  sed -i "s|%delta_y%|${delta_y}|g"      SourceAndRequests.tmp
  sed -i "s|%delta_z%|${delta_z}|g"      SourceAndRequests.tmp
   
  # - sample period index, start and end times
  sed -i "s|%SamplePeriod%|${SamplingPeriod_Index}|g"        SourceAndRequests.tmp
  sed -i "s|%SamplePeriod_Start%|${SamplingPeriod_Start}|g"  SourceAndRequests.tmp
  sed -i "s|%SamplePeriod_End%|${SamplingPeriod_Stop}|g"     SourceAndRequests.tmp
    
  # - number of particles released by each source
  sed -i "s|%nParticlesPerSource%|${ParticlesPerSource}|g"   SourceAndRequests.tmp
   
  # - duration of back run in hours
  sed -i "s|%RunDuration%|${RunDuration}|g"                  SourceAndRequests.tmp
  
  # - number of integrating times for output fields
  sed -i "s|%nIntTimes%|${nIntTimes}|g"                      SourceAndRequests.tmp
  
  # append the processed source and output requests to the main input file
  echo ''                   >> ${input_file}
  cat SourceAndRequests.tmp >> ${input_file}
  
  # remove temporary file
  rm SourceAndRequests.tmp
  
  cp  "${SCRIPTDIR}/MetOutputRequests_satellite.txt"  MetSourceAndRequests.tmp
  
  sed -i "s|%SamplePeriod%|${SamplingPeriod_Index}|g"        MetSourceAndRequests.tmp
  sed -i "s|%SamplePeriod%|${SamplingPeriod_Index}|g"        MetSourceAndRequests.tmp
  sed -i "s|%SamplePeriod%|${SamplingPeriod_Index}|g"        MetSourceAndRequests.tmp
  sed -i "s|%SourceLoc_X%|${SourceLoc_X}|g"      MetSourceAndRequests.tmp
  sed -i "s|%SourceLoc_Y%|${SourceLoc_Y}|g"      MetSourceAndRequests.tmp
  sed -i "s|%SourceLoc_Z%|${SourceLoc_Z}|g"      MetSourceAndRequests.tmp
  #sed -i "s|%vert_level%|${vert_level}|g"      MetSourceAndRequests.tmp
  sed -i "s|%vert_level%|${label}|g"      MetSourceAndRequests.tmp
  sed -i "s|%cur_date_as_char%|${cur_date_as_char}|g"      MetSourceAndRequests.tmp
  sed -i "s|%StopTimeOfRun%|${SamplingPeriod_Stop}|g"      MetSourceAndRequests.tmp      
  
  # append the processed source and output requests to the main input file
  echo ''                   >> ${met_input_file}
  cat MetSourceAndRequests.tmp >> ${met_input_file}
  
  # remove temporary file
  rm MetSourceAndRequests.tmp
    
done # corresponds to for loop
  
# ----------------------------------------------------------------------------
# Prepare the met declaration file
# ----------------------------------------------------------------------------

# copy template for met declaration file for processing
cp  ${MetDeclnFile}  MetDeclaration.tmp

# substitute dynamic variables for the met declaration

# - Path of local met directory
sed -i "s|%MetDir%|${METDIR}|g"                      MetDeclaration.tmp

# - Path of topography directory
sed -i "s|%TopogDir%|${TOPOGDIR}|g"                  MetDeclaration.tmp

# - Met restore script to use
sed -i "s|%MetRestoreScript%|${MetRestoreScript}|g"  MetDeclaration.tmp

# append the processed met declaration to the main input file
echo ''                >> ${input_file}
cat MetDeclaration.tmp >> ${input_file}

# remove temporary file
rm MetDeclaration.tmp
  
# ----------------------------------------------------------------------------
# Run NAME for this day
# ----------------------------------------------------------------------------
  
echo "=============================="
echo "Running NAME for ${cur_date}"
echo "=============================="

${NAMEIIIDIR}/Executables_Linux/nameiii_64bit_par.exe  ${input_file}

# ----------------------------------------------------------------------------
# Check for errors in NAME run
# ----------------------------------------------------------------------------

if [[ -r ${error_file} ]] ; then
  error_status=`cat ${error_file} | wc -c`
else
  error_status=1
fi

if [[ ${error_status} -ne 0 ]] ; then
  
  echo "NAME error when running: ${input_file}" >> ${WORKDIR}/MainErrors.txt
  echo "Error file is reproduced below"         >> ${WORKDIR}/MainErrors.txt
  cat ${error_file}                             >> ${WORKDIR}/MainErrors.txt
  
fi
  
# ----------------------------------------------------------------------------
# Tidy up following the NAME run
# ----------------------------------------------------------------------------
  
# # remove old met data no longer needed (older than prescribed history)
#old_met_date=`date -u -d "${cur_date} UTC -${MetHistory} day" +'%Y%m%d'`
#old_met_files="${MetPrefix[${GlobalMk}]}${old_met_date}*${MetSuffix[${GlobalMk}]}"
  
#echo "Deleting old met files ${old_met_files}"
#find ${METDIR} -name "${old_met_files}" -type f -delete
  
#change timestamp of fields files to be the release time rather than end time
mv -f Fields_grid1_* Fields_${RunName}.txt
mv -f Fields_gridBL_* Fields_BL_${RunName}.txt
gzip Fields*.txt

mv particle_location.txt particle_location_${RunName}.txt    
gzip particle_location_${RunName}.txt 
    

echo "Script $0 completing at" `date`

# -------------------------------- END OF FIELDS GENERATION -------------------------------

# --------------------------------- GENERATE MET DATA -------------------------------------
  
# copy template for met declaration file for processing
cp  ${MetDeclnFile}  MetDeclaration.tmp

# substitute dynamic variables for the met declaration

# - Path of local met directory
sed -i "s|%MetDir%|${METDIR}|g"                      MetDeclaration.tmp

# - Path of topography directory
sed -i "s|%TopogDir%|${TOPOGDIR}|g"                  MetDeclaration.tmp

# - Met restore script to use
sed -i "s|%MetRestoreScript%|${MetRestoreScript}|g"  MetDeclaration.tmp

# append the processed met declaration to the main input file
echo ''                >> ${met_input_file}
cat MetDeclaration.tmp >> ${met_input_file}

# remove temporary file
rm MetDeclaration.tmp


if [ ! -z ${NAMEIIIDIR_MET} ]
then
  echo "Using ${NAMEIIIDIR_MET} as NAME met directory"
  ${NAMEIIIDIR_MET}/Executables_Linux/nameiii_64bit_par.exe  ${met_input_file}
else
  ${NAMEIIIDIR}/Executables_Linux/nameiii_64bit_par.exe  ${met_input_file}
fi
  

gzip Met_*
rm -f particle_location.txt
mkdir Met_${RunName}
mv -f Met*.gz Met_${RunName}/.

mkdir -p ${RESULTDIR}/${FolderName} || exit 1
mkdir -p ${RESULTDIR}/${FolderName}/Input_files
mkdir -p ${RESULTDIR}/${FolderName}/Fields_files
mkdir -p ${RESULTDIR}/${FolderName}/Particle_files
mkdir -p ${RESULTDIR}/${FolderName}/Met
mkdir -p ${RESULTDIR}/${FolderName}/Processed_Fields_files

mv -f Fields*gz ${RESULTDIR}/${FolderName}/Fields_files/.              || exit 1
mv -f particle_location*gz ${RESULTDIR}/${FolderName}/Particle_files/. || exit 1
mv -f Met_${RunName}/ ${RESULTDIR}/${FolderName}/Met/.                 || exit 1
mv -f *.txt ${RESULTDIR}/${FolderName}/Input_files/.                   || exit 1

# Check if boundary layer run exists (not the case when running over just mol fractions) before copying.
check_bl=$(find . -name "Fields*BL*gz" -print)
if [[ ! -z $check_bl ]]
then
	mkdir -p ${RESULTDIR}/${FolderName}/Fields_files_BL
	mv -f Fields*BL*gz ${RESULTDIR}/${FolderName}/Fields_files_BL/.
fi

echo $retrieval_secs > ${RESULTDIR}/${FolderName}/time_step.txt
  
rm -rf ${WORKDIR}
  
find ${RESULTDIR}/${FolderName}/ -type f -print0 | xargs -0 chmod 664
  
# ----------------------------------------------------------------------------
# --------------------------------- CLEAN UP FOLDERS ---------------------------------

# rm -rf ${METDIR}

fi # corresponds to if statement if there are no data points in the day
exit 0
